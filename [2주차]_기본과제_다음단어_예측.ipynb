{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmdR6+yLNXniEXzEpEF9EQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyunMiPark/study_AI/blob/main/%5B2%EC%A3%BC%EC%B0%A8%5D_%EA%B8%B0%EB%B3%B8%EA%B3%BC%EC%A0%9C_%EB%8B%A4%EC%9D%8C%EB%8B%A8%EC%96%B4_%EC%98%88%EC%B8%A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bxpRZaSYwy5J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb283a02-7e7f-4c3f-b0bd-ae91a4e9543b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.11/dist-packages (0.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets sacremoses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last word prediction dataset 준비"
      ],
      "metadata": {
        "id": "44F1oG_QEqgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch #Pytorch의 기본 패키지(딥러닝 모델을 만들때 사용)\n",
        "from datasets import load_dataset #huggingFace datasets 라이브러리에서 데이터셋을 불러오는 함수\n",
        "from torch.utils.data import DataLoader #Pytorch의 데이터 로더, 배치 단위로 데이터를 불러올 때 사용\n",
        "from torch.nn.utils.rnn import pad_sequence #시퀀스 길이를 맞춰주는 함수(문장을 같은 길이로 패딩할 때 사용)\n",
        "from transformers import BertTokenizerFast #빠른 토크나이징을 지원하는 버전\n",
        "from tokenizers import ( #토큰화 과정에서 필요한 다양한 도구들(직접 커스텀 토크나이저를 만들 때 사용 됨)\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "\n",
        "#IMDB 데이터셋 로드 : 데이터 셋중 5%의 데이터만 사용 -> 훈련 속도를 높이기 위해 데이터 일부만 선택\n",
        "train_ds = load_dataset(\"stanfordnlp/imdb\", split=\"train[:5%]\")\n",
        "test_ds = load_dataset(\"stanfordnlp/imdb\", split=\"test[:5%]\")\n",
        "\n",
        "#Hugging Fae에서 BERT 토크나이저 로드\n",
        "#bert-base-uncased : 영어 소문자 버전의 BERT 모델 -> 이제 텍스트 데이터를 BERT 모델이 이해할 수 있는 숫자 토큰으로 변환 가능\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
        "\n",
        "\n",
        "#데이터 전처리\n",
        "def collate_fn(batch):\n",
        "  max_len = 400\n",
        "  texts, labels = [], []\n",
        "  for row in batch:\n",
        "    #마지막 세번째 토큰값을 정답으로 사용 -> 문장의 마지막 단어를 예측하는 모델을 만들려는 것\n",
        "    labels.append(tokenizer(row['text'], truncation=True, max_length=max_len).input_ids[-3])\n",
        "    #마지막 세번째 토큰을 제외한 나머지를 입력 데이터로 사용 -> 마지막 단어를 제외하고 이전 단어들을 모델 입력값으로 사용하는 것\n",
        "    texts.append(torch.LongTensor(tokenizer(row['text'], truncation=True, max_length=max_len).input_ids[:-3]))\n",
        "\n",
        "  #문장의 길이를 맞추기 위해 padding 추가(PAD 토큰)\n",
        "  texts = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "  #정답 레이블을 텐서로 변환\n",
        "  labels = torch.LongTensor(labels)\n",
        "\n",
        "  #(입력문장, 정답) 반환\n",
        "  return texts, labels\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_ds, batch_size=64, shuffle=False, collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "gimoiqAs9-l9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b684547-51c8-4034-82d4-b1ce43d09bd3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function 및 classifier output 변경\n",
        "- 마지막 token id를 예측하는 것이기 때문에 binary classification이 아닌 일반적인 classification 문제로 바뀝니다. MNIST 과제에서 했던 것 처럼 loss와 TextClassifier의 출력 차원을 잘 조정하여 task를 풀 수 있도록 수정하시면 됩니다."
      ],
      "metadata": {
        "id": "FvaTkjWcDoWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn #Pytorch의 신경망 모듈\n",
        "from math import sqrt #루트연산을 위한 함수\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, input_dim, d_model, n_heads):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim #입력 차원\n",
        "    self.d_model = d_model #모델 차원(임베딩)\n",
        "    self.n_heads = n_heads #Attention 헤드 개수\n",
        "\n",
        "    #입력 데이터를 Q(Query),K(Key),V(Value) 벡터로 변환하는 역할\n",
        "    self.wq = nn.Linear(input_dim, d_model)\n",
        "    self.wk = nn.Linear(input_dim, d_model)\n",
        "    self.wv = nn.Linear(input_dim, d_model)\n",
        "    #Attention의 최종 출력을 다시 d_model 차원으로 변환\n",
        "    self.dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "    #Attention 가중치를 확률값으로 변환\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    #x:입력텐서, mask:마스킹 정보(Attention 특정 단어를 무시할 때 사용)\n",
        "    #B(batch size) S(Sequence Length) D(각 Attention 헤드의 차원)\n",
        "    q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
        "    B, S, D = q.shape[0], q.shape[1], self.d_model // self.n_heads\n",
        "\n",
        "    #reshape:d_model을 여러개의 n_heads로 분할\n",
        "    #transpose:차원 순서를 변경(B, H, S, D)\n",
        "    q = q.reshape((B, S, self.n_heads, D)).transpose(1, 2)\n",
        "    k = k.reshape((B, S, self.n_heads, D)).transpose(1, 2)\n",
        "    v = v.reshape((B, S, self.n_heads, D)).transpose(1, 2)\n",
        "\n",
        "    #행렬곱 -> Attention Score(유사도) 계산\n",
        "    score = torch.matmul(q, k.transpose(-1, -2)) # (B, H, S, D) * (B, H, D, S) = (B, H, S, S)\n",
        "    #차원의 제곱근으로 나누어 값의 크기를 조정(스케일링)\n",
        "    score = score / sqrt(self.d_model)\n",
        "\n",
        "    #mask가 존재하면 마스킹된 부분을 매우 작은 값으로 설정\n",
        "    #Softmax에서 0으로 만듦 -> 패딩된 부분이 Attention Score에 영향을 주지 않도록 방지\n",
        "    if mask is not None:\n",
        "      score = score + (mask[:, None] * -1e9)\n",
        "\n",
        "    #Attention 가중치를 확률값으로 변환\n",
        "    score = self.softmax(score)\n",
        "    #가중치를 V벡터에 곱해 최종 값 계산\n",
        "    result = torch.matmul(score, v) #(B, H, S, D)\n",
        "\n",
        "    #ranspose:다시 원래 차원 순서로 변경\n",
        "    #reshape:멀티 헤드 결과를 다시 합쳐서 하나의 텐서로 변환\n",
        "    result = result.transpose(1, 2).reshape(B, S, -1)\n",
        "    #최종 출력을 d_model차원으로 변환\n",
        "    result = self.dense(result)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "tKLAyUV6DtEP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformer의 한 층(layer)을 구현\n",
        "class TransformerLayer(nn.Module):\n",
        "  def __init__(self, input_dim, d_model, n_heads, dff):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim #입력 차원\n",
        "    self.d_model = d_model #모델 차원\n",
        "    self.n_heads = n_heads #멀티 헤드 Attention에서 헤드의 개수\n",
        "    self.dff = dff #Feed Forward Network(FFN)의 은닉층 차원\n",
        "\n",
        "    #입력을 Q,K,V로 변환 후 Attention 수행하여 중요도를 반영한 출력을 생성\n",
        "    self.sa = MultiHeadAttention(input_dim, d_model, n_heads)\n",
        "    self.ffn = nn.Sequential(\n",
        "      nn.Linear(d_model, dff), #차원 확장\n",
        "      nn.ReLU(), #비선형 활성화 함수 적용\n",
        "      nn.Linear(dff, d_model) #다시 원래 차원(d_model)로 축소\n",
        "    )\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(d_model) #정규화 -> 출력의 안정성을 높이기 위해\n",
        "    self.dropout1 = nn.Dropout(0.1) #과적합 방지\n",
        "\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.dropout2 = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    #x:입력텐서, mask:마스킹 정보(Attention 특정 단어를 무시할 때 사용)\n",
        "    x1 = self.sa(x, mask) #Attention 수행\n",
        "    x1 = self.dropout1(x1)\n",
        "    x1 = self.norm1(x + x1) #잔차 연결(Residual Connection) 후 정규화\n",
        "\n",
        "    x2 = self.ffn(x1)\n",
        "    x2 = self.dropout2(x2)\n",
        "    x2 = self.norm2(x1 + x2)\n",
        "\n",
        "    return x2"
      ],
      "metadata": {
        "id": "i9MWj8CWkFwr"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np #수치 연산\n",
        "\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    #pos:위치값(position index), i:임베딩 차원의 index, d_model:임베딩 차원의 크기\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    #positon:최대 시퀀스 길이(max_len), d_model:임베딩 차원의 크기\n",
        "    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n",
        "    #짝수 index -> sin 함수 적용\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    #홀수 index -> cos 함수 적용\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[None, ...]\n",
        "\n",
        "    return torch.FloatTensor(pos_encoding)\n",
        "\n",
        "\n",
        "max_len = 400\n",
        "print(positional_encoding(max_len, 256).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NSYdXkCkLiN",
        "outputId": "5019e471-d24d-4dc3-e2cc-a7014494bfc2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 400, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#텍스트 분류 모델을 구현\n",
        "class TextClassifier(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, n_heads, n_layers, dff):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size #단어 사전 크기\n",
        "    self.d_model = d_model #모델의 임베딩 차원\n",
        "    self.n_heads = n_heads #멀티 헤드 Attention에서 헤드 개수\n",
        "    self.n_layers = n_layers #Transformer 인코더 총 개수\n",
        "    self.dff = dff #Feed Forward netwofk(FFN) 은닉층 크기\n",
        "\n",
        "    #vocab_size 크기의 임베딩 테이블을 생성하여 단어를 고정된 크기의 벡터로 변환\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    #requires_grad=False:위치 인코딩은 학습되지 않도록 고정\n",
        "    self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n",
        "    #n_layers 개수만큼 Transformer 인코더 레이어 쌓기\n",
        "    self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, n_heads, dff) for _ in range(n_layers)])\n",
        "    #[CLS] 토큰을 사용하여 1차원(이진 분류) 출력\n",
        "    self.classification = nn.Linear(d_model, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    mask = (x == tokenizer.pad_token_id)\n",
        "    mask = mask[:, None, :]\n",
        "    seq_len = x.shape[1]\n",
        "\n",
        "    x = self.embedding(x) #단어를 임베딩 벡터로 변환\n",
        "    x = x * sqrt(self.d_model) #임베딩 값을 조정\n",
        "    x = x + self.pos_encoding[:, :seq_len] #위치 정보 추가\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "\n",
        "    x = x[:, 0] #첫번째 토큰[CLS]을 사용하여 문장 전체를 대표하는 벡터로 활용\n",
        "    x = self.classification(x) #(batch_size, 1)형태로 생성\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "model = TextClassifier(len(tokenizer), 32, 4, 5, 32)"
      ],
      "metadata": {
        "id": "vjaETKWskR97"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 결과 report\n",
        "- 기존 Transformer 실습에서 사용한 모델로 last word prediction을 학습하고 학습 경과를 report하면 됩니다."
      ],
      "metadata": {
        "id": "DhFELLrfDzuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "lr = 0.001\n",
        "#model = model.to('cuda')\n",
        "#이진분류(Binary Classification)에서 사용\n",
        "#내부적으로 sigmoid함수 포함\n",
        "#즉 모델의 출력값을 확률로 변환할 필요 없이 logits형태 그대로 사용 가능\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "_2xXtoiVkYL3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def accuracy(model, dataloader):\n",
        "  cnt = 0\n",
        "  acc = 0\n",
        "\n",
        "  for data in dataloader:\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "    preds = model(inputs)\n",
        "    # preds = torch.argmax(preds, dim=-1)\n",
        "    preds = (preds > 0).long()[..., 0]\n",
        "\n",
        "    cnt += labels.shape[0]\n",
        "    acc += (labels == preds).sum().item()\n",
        "\n",
        "  return acc / cnt"
      ],
      "metadata": {
        "id": "YNgj_JulkegT"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  total_loss = 0.\n",
        "  model.train()\n",
        "  for data in train_loader:\n",
        "    model.zero_grad()\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda').float()\n",
        "\n",
        "    preds = model(inputs)[..., 0]\n",
        "    loss = loss_fn(preds, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    train_acc = accuracy(model, train_loader)\n",
        "    test_acc = accuracy(model, test_loader)\n",
        "    print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "suHR2ifQkfjw",
        "outputId": "af56fbb1-6a24-42ea-ad71-a812c5309e45"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "result type Float can't be cast to the desired output type Long",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-b6bfee717ba2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         return F.binary_cross_entropy_with_logits(\n\u001b[0m\u001b[1;32m    822\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3641\u001b[0m         )\n\u001b[1;32m   3642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3643\u001b[0;31m     return torch.binary_cross_entropy_with_logits(\n\u001b[0m\u001b[1;32m   3644\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3645\u001b[0m     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: result type Float can't be cast to the desired output type Long"
          ]
        }
      ]
    }
  ]
}