{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmSHkslkn0nN"
   },
   "source": [
    "DistilBERT fine-tuning으로 감정 분석 모델 학습하기\n",
    "이번 실습에서는 pre-trained된 DistilBERT를 불러와 이전 주차 실습에서 사용하던 감정 분석 문제에 적용합니다. 먼저 필요한 library들을 불러옵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk8zN8Sfn5sK"
   },
   "source": [
    "### 과제 요구 사항들을 구현하고, epoch마다의 train loss와 최종 모델의 test accuracy가 print된 notebook을 public github repository에 업로드하여 공유해주시면 됩니다. 반드시 출력 결과가 남아있어야 합니다.\n",
    "- [o] AG_News dataset 준비\n",
    "    - Huggingface dataset의 `fancyzhx/ag_news`를 load\n",
    "    - `collate_fn` 함수에 다음 수정사항들을 반영\n",
    "    - Truncation과 관련된 부분들을 삭제\n",
    "- [o] Classifier output, loss function, accuracy function 변경\n",
    "    - 뉴스 기사 분류 문제는 binary classification이 아닌 일반적인 classification 문제입니다. MNIST 과제에서 했던 것 처럼 `nn.CrossEntropyLoss` 를 추가하고 `TextClassifier`의 출력 차원을 잘 조정하여 task를 풀 수 있도록 수정\n",
    "    - 그리고 정확도를 재는 `accuracy` 함수도 classification에 맞춰 수정\n",
    "- [o]  학습 결과 report\n",
    "    - DistilBERT 실습과 같이 매 epoch 마다의 train loss를 출력하고 최종 모델의 test accuracy를 report 첨부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBCwfVr6nyU2",
    "outputId": "e0ffb85f-687f-4c23-cc7f-644bbbc24a55"
   },
   "outputs": [],
   "source": [
    "!pip install tqdm boto3 requests regex sentencepiece sacremoses datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CppLX-lfoD0n"
   },
   "source": [
    "그 후, 우리가 사용하는 DistilBERT pre-training 때 사용한 tokenizer를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326,
     "referenced_widgets": [
      "8c6febc820b84c1c822faeb13acb2fd4",
      "dafe414db1f046ffb8bc792c23431845",
      "088c2afab62844b4aaa4d60187ea23ba",
      "c8cadf5910274c77a0e328cf4fbad746",
      "82b081867ece44f8bb8265a88e743f75",
      "ed2cbf6f5e934e3eb480a12eb3ee9528",
      "124d1051ac9b43cc8ea83551e972df23",
      "fa7fa7dac3e749f0912bfb0761766795",
      "baeb271ca1cc4c5fb8bd201abfeac461",
      "fb9bdaed4a1942abbce0472ee9550a51",
      "73199ac99cac4f9ea9e3102a8bd991da",
      "8429accf7f594402a0092211eadf1dd0",
      "1f92ac63d3c040eca78500e24a2cce1f",
      "9906098f212a4f41b6ce292165ca8d82",
      "28770fa1ed97460cb487aa4197b022e5",
      "b6461d3ee17a4be6b7bf7ca3761a3789",
      "7782624a71bc47569acc73ac775535a7",
      "1bbd5ed157da4aedb2e4e86747382a06",
      "245f91d5f1b84c11b4b8b089b818f333",
      "d31757ebaac74cd88b7fa5507faec82e",
      "9ee705371ad6409b9ba44f868467ac1e",
      "6d9b7f74d2e641bb9430e6942acfac08",
      "44dd67e00d884a73a724856b8e9e8850",
      "58eb29630f8e4e67a2eaa00d3f82de2b",
      "61cb6cdd88a149ae95e39a913c94d0c3",
      "e12ec2ccf2994d35beb5b193ae3dcdb3",
      "de1c307c79f8404e996ba1f86e963257",
      "fdab9b6d369a46ecbdde45e0616a7ff2",
      "931bd8be9c1d40f28b1681ecca6c4829",
      "e2abc175a01b40a48f4eccfa1f7392f5",
      "2fee469141ee4df696b0ad5033179d18",
      "3a8627a3c1474694a25f720d4efb172e",
      "a97e0b2b60d24d109b5a5185b82e69a7",
      "58f15bf013b84754ba25aa18023d7ffe",
      "2b5973718a9c43b6a38b7c9799083862",
      "ce54bd31394640fabdba5421831439de",
      "278d4d8fc485480280920e6a2a5554ae",
      "b2d7c28fcf3143399ff4a59e64ab9568",
      "cb9a80b937aa47c8950ef477348749c7",
      "9c8e381d1f4745eab7caf9cb610c3086",
      "91284e2137444398a1e5a97c17b724ef",
      "b24e8ea477e6446b84f3c333f7be971b",
      "1a4585fc788540078beec134a6a626e2",
      "4891ea7cd58b48b2a7d32829a5d414ce"
     ]
    },
    "id": "FoDmWnQAoA4F",
    "outputId": "7ff9675d-203f-4023-ac21-2b3f3c9b9879"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# DistilBERT 모델용 tokenizer 로드 (pretrained)\n",
    "# 이 tokenizer는 문장을 토큰화해서 모델이 이해할 수 있는 input_ids로 변환해줌\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYph-EE7oNPw"
   },
   "source": [
    "DistilBERT의 tokenizer를 불러왔으면 이제 collate_fn과 data loader를 정의합니다. 이 과정은 이전 실습과 동일하게 다음과 같이 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324,
     "referenced_widgets": [
      "f758ee477b99418ba9da3c0333fd2a75",
      "1af21a3521ab40f198fc515f2cdbf040",
      "b5e343606b374c739326e8bf4fc54a7f",
      "b3e989f3d5444e6da0a30e3d45704bf5",
      "b595091aaadd4b78850d53d406c84a6f",
      "4ac74450c4e34af59e88bed23fdd5e3c",
      "3fab1b7a085249cdb7938d869cdcb9bd",
      "60efb06af5204a27b7bcfd3ae51b1fdb",
      "c23f064b5fc44d6a9280e1c67dc959dd",
      "270611d637ff4601ba196dd43dc1ab26",
      "31005a2bb63f47b391be8e42da33c42b",
      "65616cf079f14aff8c951a89efca7f60",
      "574f39d0126447cfa4b51da5d8934b9b",
      "106a6dfaa28b439c8566df7d07ac07e1",
      "4c1bb2c7d832453a84c7348f94c3d945",
      "d0ed1c6d8f524f60be3acc0c0113cb1a",
      "d7ae77e102984de281f608b5bde828f1",
      "2379ab7baf8a486aadf4ce410922dd32",
      "f323b068b98541ea9c3a34c2a88e9373",
      "dc058e7c2f1346c9ab69b8c8df1b02af",
      "71f10057b90e4b8abc9770dc0b658198",
      "8a7ddc1a1ebd48ffbe564806dfdc6b20",
      "7f88ccc30a6b4ec3a08aba6dfe5599f3",
      "d98042f121a34aef8d651575a61f0172",
      "33312b9beef1422ab19fab4c5db9107f",
      "96c076f59f04459eb87ed06e078ec824",
      "da643b54d2534e3a8f63e41a5de48da3",
      "9554d40e1f744fc9a13792b01bab6542",
      "661bd842f91d4258ba1a04f825a209e4",
      "c1f3c89e41164365a1406dc82ec162e1",
      "722e05c8fe4f4a869a063cfb8409cdcd",
      "a739417e64d740b8b1f1c9e55b622f0a",
      "964b3068be8e4ccabc91262192159d60",
      "eacd13d489184907a731d4fb5a3f924c",
      "2dba863cf5ff4c7b8b1e06185ce3b1d8",
      "547a8cb57a38411787546d27c30b8827",
      "28024c0bc48549ea89cf9f6fe242f997",
      "c41e5f7800c8497691522fdf40f65349",
      "f6dabffd741045c194b56eea5b8fcb16",
      "509ec8e643994aa7b7bc9607221ca1dc",
      "8894f25daa924f7695d5f2a37f4cc533",
      "9e21f00f460a40ce9a1d4a56dcfa2022",
      "bfca2b968aa243fca5628e0acef577e3",
      "356949a215c64c458e625ff26ef3d65c",
      "545961e820174d828f748c20ea915cb8",
      "7983dc88109a47cc9d75eda8de0a738c",
      "98ee114d0e4f4005883d38d7049ec8a7",
      "288285073c144d16af07f9e6e4d3762d",
      "314c692273a24952bdf0abe789770575",
      "19905e7a077b44e7a8b286ecff96e027",
      "41a0872360a74d9595e77651c2aa98dc",
      "9da0b32ca11f49a687401e9e4424b1e2",
      "252b465edec148c58965a3e0b22ffb97",
      "2d1f018e8f5443f486e4c061badbe18b",
      "4ef9d2d09ff644039982a38d43ba2010"
     ]
    },
    "id": "oUFf6U7joPAR",
    "outputId": "37a04e19-f6a8-4360-93bf-461d13ff4ec1"
   },
   "outputs": [],
   "source": [
    "# IMDB 감정 분석 데이터셋의 5%만 로드 (학습 데이터와 테스트 데이터 각각)\n",
    "train_ds = load_dataset(\"fancyzhx/ag_news\", split=\"train[:5%]\")\n",
    "test_ds = load_dataset(\"fancyzhx/ag_news\", split=\"test[:5%]\")\n",
    "\n",
    "#첫번째 샘플 데이터 확인\n",
    "print(\"train sample: \", train_ds[0])\n",
    "print(\"test sample:\", test_ds[0])\n",
    "\n",
    "#샘플 개수 확인\n",
    "print(\"train sample count: \", len(train_ds))\n",
    "print(\"test sample count: \", len(test_ds))\n",
    "\n",
    "# 데이터를 배치로 묶기 위한 함수 정의\n",
    "def collate_fn(batch):\n",
    "    texts, labels = [], []  # 입력 문장들과 라벨들을 저장할 리스트\n",
    "\n",
    "    # 배치 내 각 샘플에 대해 text와 label 추출\n",
    "    for row in batch:\n",
    "        labels.append(row['label'])\n",
    "        texts.append(row['text'])\n",
    "\n",
    "    # tokenizer로 텍스트를 토큰화하고, 최대 길이로 패딩 및 자르기\n",
    "    # tokenizer는 사전에 정의되어 있어야 함 (예: tokenizer = AutoTokenizer.from_pretrained(...))\n",
    "    texts = torch.LongTensor(tokenizer(texts, padding=True).input_ids)\n",
    "    # 라벨 리스트를 LongTensor로 변환\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    # 모델 학습에 필요한 입력 (토큰화된 문장들)과 정답 라벨 반환\n",
    "    return texts, labels\n",
    "\n",
    "# 학습용 DataLoader 정의 (shuffle=True로 배치 순서 랜덤화)\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# 테스트용 DataLoader 정의 (shuffle=False로 배치 순서 고정)\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=64, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejq75QcyoUxz"
   },
   "source": [
    "이제 pre-trained DistilBERT를 불러옵니다. 이번에는 PyTorch hub에서 제공하는 DistilBERT를 불러봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676,
     "referenced_widgets": [
      "0327de87ada04fec9c9dbbfb8f0d3d10",
      "ac99516279f847ff9b6d25291e5737b5",
      "a7b4608107694bb8bc24b7b3f0825798",
      "25a84962be1e4ffc90d31653f6ddbb0a",
      "e1d94e73ab6c4197afc422d3893eb748",
      "335193818da042b08f588c81666830bd",
      "2e8796100044436c9dada90937d8f38a",
      "ec42e10630314d9ca8508ae28184f560",
      "432514e3b6c746c3bdd83881211d19c7",
      "c7f9f4a5a8b042fb97e5d960ed4d0830",
      "6a0a786d9962478fa281b22b4e1329a8"
     ]
    },
    "id": "mdHWq0-LoWZ_",
    "outputId": "5ce2509c-abb8-442c-a67b-77dae4ec07b7"
   },
   "outputs": [],
   "source": [
    "#DistilBERT 모델을 PyTorch Hub에서 로드 후 model 출력\n",
    "model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ALCiqT0oYtH"
   },
   "source": [
    "출력 결과를 통해 우리는 DistilBERT의 architecture는 일반적인 Transformer와 동일한 것을 알 수 있습니다. Embedding layer로 시작해서 여러 layer의 Attention, FFN를 거칩니다.\n",
    "\n",
    "이제 DistilBERT를 거치고 난 [CLS] token의 representation을 가지고 text 분류를 하는 모델을 구현합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0cyDj_Kcoa11",
    "outputId": "16064333-f587-4039-ceb7-bb13d774fb76"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# 텍스트 분류 모델 정의 (DistilBERT + Linear layer)\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 사전학습된 DistilBERT 모델을 encoder로 불러옴 (pretrained transformer)\n",
    "        self.encoder = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
    "\n",
    "\n",
    "        # [CLS] 토큰 분류기 정의\n",
    "        self.classifier = nn.Linear(768, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder에 input_ids 전달\n",
    "        x = self.encoder(x)['last_hidden_state']\n",
    "\n",
    "        # [CLS] 토큰 위치 벡터를 classification head에 전달\n",
    "        x = self.classifier(x[:, 0])\n",
    "\n",
    "        return x  # logit 출력\n",
    "\n",
    "model = TextClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNQdgtOpojah"
   },
   "source": [
    "위와 같이 TextClassifier의 encoder를 불러온 DistilBERT, 그리고 classifier를 linear layer로 설정합니다. 그리고 forward 함수에서 순차적으로 사용하여 예측 결과를 반환합니다.\n",
    "\n",
    "다음은 마지막 classifier layer를 제외한 나머지 부분을 freeze하는 코드를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBrZxLxpokUF"
   },
   "outputs": [],
   "source": [
    "for param in model.encoder.parameters():\n",
    "  param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WYlxxIRol44"
   },
   "source": [
    "위의 코드는 encoder에 해당하는 parameter들의 requires_grad를 False로 설정하는 모습입니다. requires_grad를 False로 두는 경우, gradient 계산 및 업데이트가 이루어지지 않아 결과적으로 학습이 되지 않습니다. 즉, 마지막 classifier에 해당하는 linear layer만 학습이 이루어집니다. 이런 식으로 특정 부분들을 freeze하게 되면 효율적으로 학습을 할 수 있습니다.\n",
    "\n",
    "마지막으로 이전과 같은 코드를 사용하여 학습 결과를 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQOXNoZforNf"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def accuracy(model, dataloader):\n",
    "    cnt = 0      # 전체 샘플 수\n",
    "    acc = 0      # 정답 개수 누적\n",
    "\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "        preds = model(inputs)  # 로짓(logit) 출력\n",
    "        preds = torch.argmax(preds, dim=-1)\n",
    "\n",
    "        cnt += labels.shape[0]  # 총 샘플 수 누적\n",
    "        acc += (labels == preds).sum().item()  # 예측이 맞은 수 누적\n",
    "\n",
    "    return acc / cnt  # 정확도 반환\n",
    "\n",
    "#학습 과정 시각화 함수\n",
    "def plot_acc(train_acc, test_acc):\n",
    "  x = np.arange(len(train_acc))\n",
    "\n",
    "  plt.plot(x, train_acc, label='train') #학습 데이터 정확도 그래프\n",
    "  plt.plot(x, test_acc, label='test') #테스트 데이터 정확도 그래프\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sAQ-I3Oqo6sa",
    "outputId": "ea1dfdf3-361a-4290-a17a-e3d80998859d"
   },
   "outputs": [],
   "source": [
    "# 학습 설정\n",
    "lr = 0.001\n",
    "model = model.to('cuda')  # 모델을 GPU로 이동\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "train_acc= []\n",
    "test_acc = []\n",
    "\n",
    "n_epochs = 30\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0.\n",
    "    model.train()  # 학습 모드 설정\n",
    "\n",
    "    for data in train_loader:\n",
    "        model.zero_grad()  # 이전 gradient 초기화\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "        preds = model(inputs)\n",
    "        loss = loss_fn(preds, labels)  # 손실 계산\n",
    "        loss.backward()  # 역전파\n",
    "        optimizer.step()  # 파라미터 업데이트\n",
    "\n",
    "        total_loss += loss.item()  # loss 누적\n",
    "\n",
    "    train_acc.append(accuracy(model, train_loader))\n",
    "    test_acc.append(accuracy(model, test_loader))\n",
    "    print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")\n",
    "\n",
    "\n",
    "plot_acc(train_acc, test_acc) #학습 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aol54ZtwphT0",
    "outputId": "9c2d8a93-9748-4d9f-8114-c81da78096d4"
   },
   "outputs": [],
   "source": [
    "# 평가 시 gradient 계산 비활성화\n",
    "with torch.no_grad():\n",
    "    model.eval()  # 평가 모드로 전환 (계산 비활성화)\n",
    "    train_acc = accuracy(model, train_loader)\n",
    "    test_acc = accuracy(model, test_loader)\n",
    "\n",
    "    print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
